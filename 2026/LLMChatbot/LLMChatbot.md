# æ„å»ºèŠå¤©æœºå™¨äºº

åœ¨å¼€å§‹ä¹‹å‰è¯·å…ˆé…ç½®å¥½Langsmithå’ŒOpenAiçš„ç¯å¢ƒå˜é‡ã€‚
èŠå¤©æ¨¡å‹æœ¬èº«æ²¡æœ‰ä»»ä½•çŠ¶æ€å­˜å‚¨çš„ã€‚
```
# ç®€å•çš„ç›´æ¥ä½¿ç”¨ChatModelæ¨¡å‹
def simple_chat():
    model = init_chat_model("gpt-4o-mini", model_provider="openai")
    response = model.invoke([HumanMessage(content="Hi! I'm Bob")])
    print(response.content)
    response = model.invoke([HumanMessage(content="What's my name?")])
    print(response.content)
```

è¿”å›æ‰“å°ç»“æœå¦‚ä¸‹ï¼š
```
Hi Bob! How can I assist you today?
I'm sorry, but I don't know your name. You haven't mentioned it yet. How can I help you today?
```
å¯ä»¥çœ‹åˆ°ï¼Œå®ƒæ²¡æœ‰å°†ä¹‹å‰çš„å¯¹è¯è½¬åŒ–ä¸ºä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”æ— æ³•å›ç­”é—®é¢˜ã€‚ è¿™ä¼šå¯¼è‡´ç³Ÿç³•çš„èŠå¤©æœºå™¨äººä½“éªŒï¼

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•´ä¸ªå¯¹è¯å†å²ä¼ é€’åˆ°æ¨¡å‹ä¸­ã€‚å¦‚ä¸‹ï¼š
```
def simple_chat_history():
    model = init_chat_model("gpt-4o-mini", model_provider="openai")
    response = model.invoke(
        [
            HumanMessage(content="Hi! I'm Bob"),
            AIMessage(content="Hello Bob! How can I assist you today?"),
            HumanMessage(content="What's my name?"),
        ]
    )
    print(response.content)
```
åˆ™ç»“æœæ˜¯`Your name is Bob! How can I help you today?`ã€‚

ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†å¾ˆå¥½çš„å›åº”ï¼è¿™æ˜¯æ”¯æ’‘èŠå¤©æœºå™¨äººè¿›è¡Œå¯¹è¯äº¤äº’èƒ½åŠ›çš„åŸºæœ¬æ€æƒ³ã€‚ä¸”èŠå¤©æ¨¡å‹å¯¹è¾“å…¥å¤§å°æœ‰æœ€å¤§é™åˆ¶ï¼Œå› æ­¤[ç®¡ç†èŠå¤©å†å²è®°å½•](https://langchain.cadn.net.cn/python/docs/concepts/chat_history/index.html)å¹¶æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œä¿®å‰ªä»¥é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£éå¸¸é‡è¦ã€‚

## æ¶ˆæ¯æŒä¹…åŒ–
[LangGraph](https://langchain-ai.github.io/langgraph/)å®ç°äº†ä¸€ä¸ªå†…ç½®çš„æŒä¹…å±‚ï¼Œä½¿å…¶æˆä¸ºæ”¯æŒå¤šä¸ªå¯¹è¯è½®æ¬¡çš„èŠå¤©åº”ç”¨ç¨‹åºçš„ç†æƒ³é€‰æ‹©ã€‚
```
pip install langgraph
```
LangGraphå¸¦æœ‰ä¸€ä¸ªç®€å•çš„[å†…å­˜æ£€æŸ¥ç‚¹](https://langchain-ai.github.io/langgraph/concepts/persistence/)ç¨‹åºï¼Œå¦‚ä¸‹ï¼š
```
def chat_with_graph():
    workflow = StateGraph(state_schema=MessagesState)
    # æ·»åŠ å•ä¸ªèŠ‚ç‚¹åˆ°Graph
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model)
    # å°†Graphæ·»åŠ åˆ°å†…å­˜ä¸­
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    # ä¼ é€’åˆ°runnableä¸­ï¼Œæ­¤é…ç½®åŒ…å«çš„ä¿¡æ¯ä¸ç›´æ¥å±äºinputï¼Œä½†å¾ˆé‡è¦ã€‚å®ƒä½¿å•ä¸ªåº”ç”¨ç¨‹åºæ”¯æŒå¤šä¸ªå¯¹è¯çº¿ç¨‹(ä¸åŒthread_idä¸åŒå¯¹è¯)
    config = RunnableConfig(configurable={"thread_id": "abc123"})

    query = "Hi! I'm Bob."
    input_messages = [HumanMessage(query)]
    output = app.invoke({"messages": input_messages}, config)
    output["messages"][-1].pretty_print()
    
    query = "What's my name?"
    input_messages = [HumanMessage(query)]
    output = app.invoke({"messages": input_messages}, config)
    output["messages"][-1].pretty_print()
    # æ‰“å°å®Œæ•´å†å²
    for i, msg in enumerate(output["messages"]):
        print(f"{i + 1}. {msg.type}: {msg.content}")
```

æ‰“å°è¾“å‡ºï¼š
```
======== Ai Message =========

Hi, Bob! How can I assist you today?

======== Ai Message =========

Your name is Bob! How can I help you today?
1. human: Hi! I'm Bob.
2. ai: Hi Bob! How can I assist you today?
3. human: What's my name?
4. ai: Your name is Bob! How can I help you today?
```

ç”±äº`MemorySaver`æ˜¯åŠ è½½åˆ°å†…å­˜ä¸­çš„ï¼Œæ‰€ä»¥é‡‡ç”¨è¿ç»­æ‰§è¡Œçš„æ–¹å¼ã€‚é‡æ–°æ‰§è¡Œå‡½æ•°ï¼Œå†…å­˜ä¼šè¢«é‡Šæ”¾ã€‚
å¯é‡‡ç”¨æ•°æ®åº“æˆ–æ–‡ä»¶å‚¨å­˜ä»£æ›¿å†…å­˜å­˜å‚¨ã€‚

åœ¨åŒä¸€å†…å­˜ä¸­ä½¿ç”¨ä¸åŒ`thread_id`åˆ™è§†ä¸ºä¸¤ä¸ªèŠå¤©çª—å£ã€‚

å¼‚æ­¥è°ƒç”¨åˆ™éœ€è¦ä¿®æ”¹`call_model`:
```
async def call_model(state: MessagesState):
    response = await model.ainvoke(state["messages"])
    return {"messages": response}
```
è°ƒç”¨ä¹Ÿéœ€è¦ä½¿ç”¨`await`ä¿®é¥°,ä¾‹å¦‚ï¼š`output = await app.ainvoke({"messages": input_messages}, config)`ã€‚

## æç¤ºæ¨¡æ¿

[æç¤ºæ¨¡æ¿](https://langchain.cadn.net.cn/python/docs/concepts/prompt_templates/index.html)æœ‰åŠ©äºå°†åŸå§‹ç”¨æˆ·ä¿¡æ¯è½¬æ¢ä¸ºLLMå¯ä»¥ä½¿ç”¨çš„æ ¼å¼.

ä¿®æ”¹ä¸Šé¢çš„ä¾‹å­ï¼Œä½¿ç”¨`ChatPromptTemplate`æç¤ºæ¨¡ç‰ˆè°ƒç”¨æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨`MessagesPlaceholder`ä»¥ä¼ å…¥æ‰€æœ‰æ¶ˆæ¯ï¼š
```
# è°ƒç”¨æ¨¡å‹
def call_model_prompt(state: MessagesState):
    model = init_chat_model("gpt-4o-mini", model_provider="openai")
    # è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯æç¤ºæ¨¡ç‰ˆï¼Œå¹¶åˆ©ç”¨MessagesPlaceholderä»¥ä¼ å…¥æ‰€æœ‰æ¶ˆæ¯
    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                # You talk like a pirate. Answer all questions to the best of your ability.
                "ä½ è¯´è¯åƒä¸€ä¸ªæµ·ç›—ã€‚å°½ä½ æ‰€èƒ½ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": response}
```

è°ƒç”¨`chat_with_graph`æ—¶ï¼Œå°†`workflow.add_node("model", call_model)`æ”¹ä¸º`workflow.add_node("model", call_model_prompt)`ï¼Œæ‰§è¡Œ`chat_with_graph()`
æˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹çš„å›ç­”ï¼š
```
================ Ai Message ====================

Ahoyï¼Œå‰å§†ï¼æ¬¢è¿ç™»èˆ¹ï¼æœ‰ä½•æŒ‡ä»¤æˆ–è€…é—®é¢˜ï¼Œå°½ç®¡é—®å§ï¼â˜ ï¸âš“ï¸
================ Ai Message ====================

ä½ å«å‰å§†ï¼Œæµ·ç›—çš„æœ‹å‹ï¼æœ‰ä»€ä¹ˆéœ€è¦æˆ‘è¿™ä½è€æµ·ç›—å¸®å¿™çš„ï¼Œå°½ç®¡è¯´å§ï¼â˜ ï¸ğŸ´â€â˜ ï¸
1. human: Hi! I'm Jim.
2. ai: Ahoyï¼Œå‰å§†ï¼æ¬¢è¿ç™»èˆ¹ï¼æœ‰ä½•æŒ‡ä»¤æˆ–è€…é—®é¢˜ï¼Œå°½ç®¡é—®å§ï¼â˜ ï¸âš“ï¸
3. human: What's my name?
4. ai: ä½ å«å‰å§†ï¼Œæµ·ç›—çš„æœ‹å‹ï¼æœ‰ä»€ä¹ˆéœ€è¦æˆ‘è¿™ä½è€æµ·ç›—å¸®å¿™çš„ï¼Œå°½ç®¡è¯´å§ï¼â˜ ï¸ğŸ´â€â˜ ï¸

```

ä¸‹é¢æˆ‘ä»¬ä¿®æ”¹æç¤ºè¯ï¼Œå¹¶æ·»åŠ è¾“å…¥å˜é‡`language`åˆ°æç¤ºç¬¦ä¸­ï¼š
```
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            # You are a helpful assistant. Answer all questions to the best of your ability in {language}.
            "ä½ æ˜¯ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚å°½ä½ æ‰€èƒ½ç”¨{language}å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```
æˆ‘ä»¬ç°åœ¨å°±éœ€è¦ä¸¤ä¸ªè¾“å…¥å‚æ•°`messages`å’Œ`language`ã€‚æ‰€ä»¥æˆ‘ä»¬å°±éœ€è¦æ‰©å±•`state_schema`:
```
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¿®æ”¹`MessagesState`ä¸º`State`, å¹¶æ·»åŠ è¾“å…¥å‚æ•°`language`å­—æ®µï¼Œä»£ç ä¿®æ”¹å¦‚ä¸‹ï¼š
```
def call_model_prompt(state: State):
    model = init_chat_model("gpt-4o-mini", model_provider="openai")
    # è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯æç¤ºæ¨¡ç‰ˆï¼Œå¹¶åˆ©ç”¨MessagesPlaceholderä»¥ä¼ å…¥æ‰€æœ‰æ¶ˆæ¯
    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                # You are a helpful assistant. Answer all questions to the best of your ability in {language}.
                "ä½ æ˜¯ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚å°½ä½ æ‰€èƒ½ç”¨{language}å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": [response]}

def chat_with_graph():
    workflow = StateGraph(state_schema=State)
    # æ·»åŠ å•ä¸ªèŠ‚ç‚¹åˆ°Graph
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model_prompt)
    workflow.add_edge("model", END)
    # å°†Graphæ·»åŠ åˆ°å†…å­˜ä¸­
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    # ä¼ é€’åˆ°runnableä¸­ï¼Œæ­¤é…ç½®åŒ…å«çš„ä¿¡æ¯ä¸ç›´æ¥å±äºinputï¼Œä½†å¾ˆé‡è¦ã€‚å®ƒä½¿å•ä¸ªåº”ç”¨ç¨‹åºæ”¯æŒå¤šä¸ªå¯¹è¯çº¿ç¨‹(ä¸åŒthread_idä¸åŒå¯¹è¯)
    config = RunnableConfig(configurable={"thread_id": "abc123"})

    query = "Hi! I'm Jim."
    language = "zh"
    input_messages = [HumanMessage(query)]
    output = app.invoke(
        {"messages": input_messages, "language": language},
        config
    )
    output["messages"][-1].pretty_print()

    query = "What's my name?"
    input_messages = [HumanMessage(query)]
    output = app.invoke(
        {"messages": input_messages, "language": language},
        config
    )
    output["messages"][-1].pretty_print()
    # æ‰“å°å®Œæ•´å†å²
    for i, msg in enumerate(output["messages"]):
        print(f"{i + 1}. {msg.type}: {msg.content}")
```

## ç®¡ç†å¯¹è¯å†å²
æ„å»ºèŠå¤©æœºå™¨äººæ—¶éœ€è¦ç®¡ç†å¯¹è¯å†å², å¦åˆ™æ¶ˆæ¯åˆ—è¡¨æ— é™å¢é•¿ï¼Œå°†ä½¿LLMä¸Šä¸‹æ–‡æº¢å‡ºã€‚å› æ­¤æ·»åŠ ä¸€ä¸ªé™åˆ¶ä¼ é€’çš„æ¶ˆæ¯å¤§å°çš„æ­¥éª¤æ˜¯å¾ˆé‡è¦çš„ã€‚

LangChainå†…ç½®äº†[ç®¡ç†æ¶ˆæ¯åˆ—è¡¨](https://python.langchain.com/docs/how_to/#messages)å·¥å…·åŠ©æ‰‹.ä½¿ç”¨ä¿®å‰ªå™¨`trim_messages`æ¥å‡å°‘æˆ‘ä»¬å‘æ¨¡å‹å‘é€çš„æ¶ˆæ¯æ•°é‡ã€‚
ä¿®å‰ªå™¨å…è®¸æˆ‘ä»¬æŒ‡å®šæˆ‘ä»¬æƒ³è¦ä¿ç•™å¤šå°‘ä¸ªä»¤ç‰Œ,ä»¥åŠå…¶ä»–å‚æ•°,ä¾‹å¦‚å¦‚æœæˆ‘ä»¬æƒ³å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ä»¥åŠæ˜¯å¦å…è®¸éƒ¨åˆ†æ¶ˆæ¯:
```
def chat_trim_messages():
    model = init_chat_model("gpt-4o-mini", model_provider="openai")
    trimmer = trim_messages(
        max_tokens=65,
        strategy="last",
        token_counter=model,
        include_system=True,
        allow_partial=False,
        start_on="human",
    )

    messages = [
        SystemMessage(content="you're a good assistant"),
        HumanMessage(content="hi! I'm bob"),
        AIMessage(content="hi!"),
        HumanMessage(content="I like vanilla ice cream"),
        AIMessage(content="nice"),
        HumanMessage(content="whats 2 + 2"),
        AIMessage(content="4"),
        HumanMessage(content="thanks"),
        AIMessage(content="no problem!"),
        HumanMessage(content="having fun?"),
        AIMessage(content="yes!"),
    ]

    output = trimmer.invoke(messages)
    # æ‰“å°å®Œæ•´å†å²
    for i, msg in enumerate(output):
        print(f"{i + 1}. {msg.type}: {msg.content}")
```